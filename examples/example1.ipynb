{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with dictionaries and word embeddings\n",
    "\n",
    "## Example\n",
    "\n",
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import pandas as pd\n",
    "\n",
    "from weelex import WEELexClassifier\n",
    "from weelex import Lexicon\n",
    "from weelex import Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  2.1 Dictionary/Lexicon \n",
    "\n",
    "- We need `Lexicon` objects.\n",
    "- These can be in different formats:\n",
    "    - tabular:\n",
    "        -  `pandas.DataFrame`where each column is one of the categories and each row is the words for that category\n",
    "        - `.csv` file path with data of the same format\n",
    "    - Key-value pairs:\n",
    "        - `dict` of the form `{'category1':['term1', 'term2'], 'category2': ['term3', 'term4', 'term5']}`\n",
    "        - `.json` file path with data of the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular data:\n",
    "df1 = pd.read_csv('examplefiles/mylex1.csv', sep=';', encoding='latin1')\n",
    "lex1 = Lexicon(df1)\n",
    "# or:\n",
    "lex1 = Lexicon('examplefiles/mylex1.csv', sep=';', encoding='latin1')\n",
    "lex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings/key-value pairs:\n",
    "lex2 = Lexicon('examplefiles/mylex2.json')\n",
    "lex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, different lexica can be combined into one, for example if different dictionary sources are considered/required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lex1.merge(lex2, inplace=False)\n",
    "lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Embeddings\n",
    "\n",
    "- Pretrained embedding vectors need to be provided. In the future, the will be support for self-training or fine tuning.\n",
    "- Pretrained FastText Vectors can be downloaded on the official website: \n",
    "    - [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "    - Here, we download the German vectors with the `bin` version.\n",
    "    - store these somewhere\n",
    "    - the file is several `GB` large $\\rightarrow$ dowloading the file and loading it into memory may take some time\n",
    "    - the file is compressed after download (`.bin.gz`). This is fine. It does not need to be uncompressed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_embeddings = '../../cc.de.300.bin'  # change this to your saved location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = Embeddings()\n",
    "embeds.load_vectors(path_to_embeddings, embedding_type='fasttext', fine_tuned=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding object can be filtered such that it only contains the words that are in the dictionary, which is sufficient for the method.\n",
    "The filtered embeddings can be saved and in subsequent sessions these can be loaded which reduces the required memory and loading times in subsequent operations.\n",
    "This is particularly valuable in case you need to work on the following steps of the classification over multiple days and sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.filter_terms(lex.vocabulary)\n",
    "\n",
    "# saving\n",
    "path_to_filtered_embeddings = './filtered_embeddings'\n",
    "embeds.save_filtered(path_to_filtered_embeddings)\n",
    "del embeds\n",
    "\n",
    "# create new embeds instance and load the filtered vectors\n",
    "embeds = Embeddings()\n",
    "embeds.load_filtered(path_to_filtered_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model on the dictionary\n",
    "\n",
    "- The method works by first training a machine learning ensemble on the dictionary.\n",
    "- It is possible to provide `main_keys`, i.e. the categories to predict, and `support_keys`, i.e. other categories you do not want a prediction for but provide terms anyhow\n",
    "- including `support_keys` can improve the classification because it allows the model to differentiate more words\n",
    "- by default, all the keys of your `Lexicon` instance are main keys. But this can be changed with the `main_keys` and `support_keys` parameter. Alternatively, it is possible to provide a `Lexicon` instance via the `lex` parameter for main categories and another `Lexicon` via the `support_lex` parameter for support categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = WEELexClassifier(embeds=embeds,\n",
    "                              relevant_pos=['NOUN'],\n",
    "                              min_df=1,  # Optional. Selected in able to run on small example. Better to have higher value. Default is 5\n",
    "                              max_df=0.99,  # Optional. Selected in able to run on small example. Default is 0.95\n",
    "                              n_docs=20,  # Optional. Selected to run on small example. Ideally, use the length of your data.\n",
    "                              n_words=10  # Optional. Selected to run on small example. Default is 40000\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tune the machine learning model, we specify a grid of hyperparameters\n",
    "# this will be searched via RandomizedSearch\n",
    "# This grid is very basic with only 6 possible combinations. It is only\n",
    "# used for this example and should be expanded upon in a real setting.\n",
    "param_grid = [{'modeltype': ['svm'],\n",
    "            'n_models': [2],\n",
    "            'pca': [10, None],\n",
    "            'svc_c': [0.1, 1, 10]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.weelexfit(lex=lex,\n",
    "                     support_lex=None,  # entire support lexicon can be passed instead of the 'support_keys' parameter\n",
    "                     main_keys=['PolitikVR', 'AutoVR'],  # optional. Uses all keys of lex if None\n",
    "                     support_keys=['Space', 'Food'],  # optional. Is not used if None\n",
    "                     hp_tuning=True,  # Hyperparameter tuning -> use for best results\n",
    "                     param_grid=param_grid,  # Hyperparameter grid for hp tuning\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Predict a body of texts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Texts to predict in this example:\n",
    "data = pd.Series(\n",
    "    [\n",
    "    'Ich esse gerne Kuchen und andere Süßigkeiten',\n",
    "    'Dort steht ein schnelles Auto mit einem Lenkrad und Reifen.',\n",
    "    'Die Politik von heute ist nicht mehr die gleiche wie damals.',\n",
    "    'Hier ist nochmal ein sehr generischer Satz.',\n",
    "    'Wie ist das Wetter heute?',\n",
    "    'Ich esse gerne Kuchen und andere Süßigkeiten',\n",
    "    'Dort steht ein schnelles Auto mit einem Lenkrad und Reifen.',\n",
    "    'Die Politik von heute ist nicht mehr die gleiche wie damals.',\n",
    "    'Hier ist nochmal ein sehr generischer Satz.',\n",
    "    'Wie ist das Wetter heute?',\n",
    "    'Ich esse gerne Kuchen und andere Süßigkeiten',\n",
    "    'Dort steht ein schnelles Auto mit einem Lenkrad und Reifen.',\n",
    "    'Die Politik von heute ist nicht mehr die gleiche wie damals.',\n",
    "    'Hier ist nochmal ein sehr generischer Satz.',\n",
    "    'Wie ist das Wetter heute?',\n",
    "    'Ich esse gerne Kuchen und andere Süßigkeiten',\n",
    "    'Dort steht ein schnelles Auto mit einem Lenkrad und Reifen.',\n",
    "    'Die Politik von heute ist nicht mehr die gleiche wie damals.',\n",
    "    'Hier ist nochmal ein sehr generischer Satz.',\n",
    "    'Wie ist das Wetter heute?',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.weelexpredict(data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('weelex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18fadc8bf522f865c89552f3439cd3acae47c651b1f907d5d8c80e137e4c93af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
